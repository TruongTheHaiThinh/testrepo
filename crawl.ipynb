{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e4a6e6-ac11-45ea-9ad6-826757315267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully crawled and saved 5 AI articles from VnExpress!\n",
      "\n",
      "Article 1:\n",
      "Title: Thiết bị 'hiểu' đất bằng AI đoạt giải về giải pháp xanh\n",
      "URL: https://vnexpress.net/thiet-bi-hieu-dat-bang-ai-doat-giai-ve-giai-phap-xanh-4870256.html\n",
      "Summary: Bộ thiết bị đo chất lượng đất bằng AI của Enfarm là một trong ba giải pháp đoạt giải Startup bền vững được yêu thích.\n",
      "Time: N/A\n",
      "Author: N/A\n",
      "\n",
      "Article 2:\n",
      "Title: 'Cha đẻ' Internet và các chuyên gia lo con người lệ thuộc AI\n",
      "URL: https://vnexpress.net/cha-de-internet-va-cac-chuyen-gia-lo-con-nguoi-le-thuoc-ai-4869747.html\n",
      "Summary: \"Cha đẻ\" Internet Vinton Cerf và nhiều chuyên gia lo ngại AI sẽ khiến con người mất đi kỹ các năng cốt lõi, như sự đồng cảm hay tư duy sâu.\n",
      "Time: N/A\n",
      "Author: N/A\n",
      "\n",
      "Article 3:\n",
      "Title: Cơn sốt 'đóng vỉ chân dung' bằng ChatGPT\n",
      "URL: https://vnexpress.net/con-sot-dong-vi-chan-dung-bang-chatgpt-4869364.html\n",
      "Summary: Tính năng tạo hộp đồ chơi figure với hình ảnh bản thân bằng ChatGPT được nhiều người hào hứng chia sẻ trên mạng xã hội.\n",
      "Time: N/A\n",
      "Author: N/A\n",
      "\n",
      "Article 4:\n",
      "Title: DeepSeek vượt ChatGPT về lượt truy cập mới hàng tháng\n",
      "URL: https://vnexpress.net/deepseek-vuot-chatgpt-ve-luot-truy-cap-moi-hang-thang-4869278.html\n",
      "Summary: DeepSeek có tốc độ tăng trưởng trên nền tảng web vượt qua ChatGPT và đang là công cụ AI được dùng nhiều thứ ba trên toàn cầu.\n",
      "Time: N/A\n",
      "Author: N/A\n",
      "\n",
      "Article 5:\n",
      "Title: Người dùng ChatGPT tăng vọt nhờ AI tạo ảnh\n",
      "URL: https://vnexpress.net/nguoi-dung-chatgpt-tang-vot-nho-ai-tao-anh-4868976.html\n",
      "Summary: Tính năng AI tạo ảnh trên ChatGPT, trong đó có phong cách Ghibli, gây sốt giúp số lượng người dùng chatbot của OpenAI tăng cao và khiến máy chủ quá tải.\n",
      "Time: N/A\n",
      "Author: N/A\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def crawl_vnexpress_ai_articles():\n",
    "    url = \"https://vnexpress.net/cong-nghe/ai\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send HTTP request\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all article items\n",
    "        articles = soup.find_all('article', class_='item-news', limit=5)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for article in articles:\n",
    "            # Extract title and URL\n",
    "            title_element = article.find('h2', class_='title-news') or article.find('h3', class_='title-news')\n",
    "            title = title_element.get_text().strip() if title_element else \"N/A\"\n",
    "            \n",
    "            link_element = title_element.find('a') if title_element else None\n",
    "            url = link_element['href'] if link_element else \"N/A\"\n",
    "            \n",
    "            # Extract summary\n",
    "            summary_element = article.find('p', class_='description')\n",
    "            summary = summary_element.get_text().strip() if summary_element else \"N/A\"\n",
    "            \n",
    "            # Extract time\n",
    "            time_element = article.find('span', class_='time')\n",
    "            time = time_element.get_text().strip() if time_element else \"N/A\"\n",
    "            \n",
    "            # Extract author (often in the summary or not available directly)\n",
    "            author_element = article.find('p', class_='author')\n",
    "            author = author_element.get_text().strip() if author_element else \"N/A\"\n",
    "            \n",
    "            # If author not found directly, sometimes it's in the description\n",
    "            if author == \"N/A\" and summary_element:\n",
    "                if \"(\" in summary and \")\" in summary:\n",
    "                    author = summary.split(\"(\")[-1].split(\")\")[0].strip()\n",
    "            \n",
    "            results.append({\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'summary': summary,\n",
    "                'time': time,\n",
    "                'author': author\n",
    "            })\n",
    "        \n",
    "        # Save to JSON file\n",
    "        with open('vnexpress_ai_articles.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(\"Successfully crawled and saved 5 AI articles from VnExpress!\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run the crawler\n",
    "articles = crawl_vnexpress_ai_articles()\n",
    "\n",
    "# Print the results\n",
    "for i, article in enumerate(articles, 1):\n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"URL: {article['url']}\")\n",
    "    print(f\"Summary: {article['summary']}\")\n",
    "    print(f\"Time: {article['time']}\")\n",
    "    print(f\"Author: {article['author']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0d6c7d-a4d8-4f8c-9785-bf43b11e3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu làm sạch dữ liệu...\n",
      "Có lỗi khi làm sạch dữ liệu: [Errno 2] No such file or directory: 'raw_articles.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Loại bỏ HTML tags và làm sạch text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    clean_text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    clean_text = ' '.join(clean_text.split())\n",
    "    \n",
    "    # Remove special characters except Vietnamese\n",
    "    clean_text = re.sub(r'[^\\w\\sàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴÈÉẸẺẼÊỀẾỆỂỄÌÍỊỈĨÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠÙÚỤỦŨƯỪỨỰỬỮỲÝỴỶỸĐ\\-.,;:?!]', '', clean_text)\n",
    "    \n",
    "    return clean_text.strip()\n",
    "\n",
    "def clean_articles():\n",
    "    try:\n",
    "        print(\"Bắt đầu làm sạch dữ liệu...\")\n",
    "        with open('raw_articles.json', 'r', encoding='utf-8') as f:\n",
    "            raw_articles = json.load(f)\n",
    "        \n",
    "        cleaned_articles = []\n",
    "        \n",
    "        for article in raw_articles:\n",
    "            cleaned_article = {\n",
    "                'title': clean_text(article['title']),\n",
    "                'url': article['url'],\n",
    "                'summary': clean_text(article['summary']),\n",
    "                'time': clean_text(article['time']),\n",
    "                'author': clean_text(article['author']),\n",
    "                'crawl_time': article['crawl_time']\n",
    "            }\n",
    "            cleaned_articles.append(cleaned_article)\n",
    "        \n",
    "        with open('cleaned_articles.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned_articles, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(f\"Đã làm sạch {len(cleaned_articles)} bài viết\")\n",
    "        return cleaned_articles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Có lỗi khi làm sạch dữ liệu: {e}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364092d-19f5-4b2e-b74d-5fdf2054a7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
